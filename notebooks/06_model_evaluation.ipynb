{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed084cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ENVIRONNEMENT D'ÉVALUATION CONFIGURÉ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adccf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHARGEMENT DES DONNÉES\n",
    "# ============================================\n",
    "\n",
    "df = pd.read_csv('../data/annotated/podcasts_annotated.csv')\n",
    "\n",
    "if 'keywords_text' not in df.columns:\n",
    "    df['keywords_text'] = df['keywords_clean'].apply(\n",
    "        lambda x: ' '.join(eval(x)) if isinstance(x, str) else ' '.join(x)\n",
    "    )\n",
    "\n",
    "X = df['keywords_text']\n",
    "y = df['is_kid_friendly']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset prêt: Train={len(X_train):,}, Test={len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENTRAÎNEMENT DES MODÈLES FINAUX\n",
    "# (utiliser les meilleurs paramètres du tuning)\n",
    "# ============================================\n",
    "\n",
    "# TODO: Remplacer par les paramètres optimaux du notebook 03\n",
    "\n",
    "best_lr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=None, min_df=2, max_df=0.95,\n",
    "        ngram_range=(1, 2), sublinear_tf=True\n",
    "    )),\n",
    "    ('clf', LogisticRegression(\n",
    "        C=1.0, max_iter=1000, random_state=42,\n",
    "        class_weight='balanced', solver='liblinear'\n",
    "    ))\n",
    "])\n",
    "\n",
    "best_svm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=None, min_df=2, max_df=0.95,\n",
    "        ngram_range=(1, 2), sublinear_tf=True\n",
    "    )),\n",
    "    ('clf', LinearSVC(\n",
    "        C=1.0, max_iter=2000, random_state=42,\n",
    "        class_weight='balanced', dual='auto'\n",
    "    ))\n",
    "])\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': best_lr,\n",
    "    'Linear SVM': best_svm\n",
    "}\n",
    "\n",
    "# Entraînement\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"✓ {name} entraîné\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MATRICES DE CONFUSION\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, (name, model) in enumerate(models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm,\n",
    "        display_labels=['Not Kid-Friendly', 'Kid-Friendly']\n",
    "    )\n",
    "    disp.plot(ax=axes[idx], cmap='Blues', values_format='d')\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix', fontsize=13, fontweight='bold')\n",
    "    axes[idx].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Matrices de confusion sauvegardées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RAPPORTS DE CLASSIFICATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RAPPORTS DE CLASSIFICATION DÉTAILLÉS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"─\" * 80)\n",
    "    print(classification_report(\n",
    "        y_test, y_pred,\n",
    "        target_names=['Not Kid-Friendly', 'Kid-Friendly'],\n",
    "        digits=4\n",
    "    ))\n",
    "    print(\"─\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COURBES ROC\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Probabilités\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_score = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_score = model.decision_function(X_test)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2.5,\n",
    "             label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "plt.title('ROC Curves - Kid-Friendly Podcast Classification', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Courbes ROC sauvegardées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALISATION MLFLOW (si disponible)\n",
    "# ============================================\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    experiment = mlflow.get_experiment_by_name(\"podcast-classification-kid-friendly\")\n",
    "    \n",
    "    if experiment:\n",
    "        runs = mlflow.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            order_by=[\"metrics.f1_score DESC\"],\n",
    "            max_results=10\n",
    "        )\n",
    "        \n",
    "        if not runs.empty:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\" TOP 10 MLFLOW RUNS (par F1-Score)\")\n",
    "            print(\"=\"*80 + \"\\n\")\n",
    "            \n",
    "            display_cols = ['tags.model_type', 'metrics.f1_score', \n",
    "                          'metrics.accuracy', 'metrics.precision', 'metrics.recall']\n",
    "            available_cols = [col for col in display_cols if col in runs.columns]\n",
    "            \n",
    "            print(runs[available_cols].head(10).to_string(index=False))\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            \n",
    "            # Visualisation\n",
    "            if 'metrics.f1_score' in runs.columns and 'tags.model_type' in runs.columns:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                plot_data = runs.dropna(subset=['metrics.f1_score', 'tags.model_type']).head(10)\n",
    "                \n",
    "                plt.barh(range(len(plot_data)), plot_data['metrics.f1_score'].values,\n",
    "                        color=plt.cm.viridis(np.linspace(0.3, 0.9, len(plot_data))))\n",
    "                \n",
    "                plt.yticks(range(len(plot_data)), plot_data['tags.model_type'].values)\n",
    "                plt.xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "                plt.title('MLflow Runs Comparison (Top 10)', \n",
    "                         fontsize=14, fontweight='bold', pad=20)\n",
    "                plt.grid(axis='x', alpha=0.3)\n",
    "                \n",
    "                for i, v in enumerate(plot_data['metrics.f1_score'].values):\n",
    "                    plt.text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig('../models/mlflow_runs_comparison.png', dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"✓ Visualisation MLflow sauvegardée\")\n",
    "        else:\n",
    "            print(\" Aucun run trouvé dans l'expérience\")\n",
    "    else:\n",
    "        print(\" Expérience MLflow introuvable\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\" MLflow non accessible: {str(e)[:60]}\")\n",
    "    print(\" → Passé (optionnel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FEATURE IMPORTANCE (Logistic Regression)\n",
    "# ============================================\n",
    "\n",
    "lr_model = models['Logistic Regression']\n",
    "tfidf = lr_model.named_steps['tfidf']\n",
    "clf = lr_model.named_steps['clf']\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coefficients = clf.coef_[0]\n",
    "\n",
    "# Top features positives (Kid-Friendly)\n",
    "top_positive_idx = np.argsort(coefficients)[-20:]\n",
    "top_positive_features = [(feature_names[i], coefficients[i]) for i in top_positive_idx]\n",
    "\n",
    "# Top features négatives (Not Kid-Friendly)\n",
    "top_negative_idx = np.argsort(coefficients)[:20]\n",
    "top_negative_features = [(feature_names[i], coefficients[i]) for i in top_negative_idx]\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Kid-Friendly features\n",
    "words_pos = [f[0] for f in top_positive_features]\n",
    "scores_pos = [f[1] for f in top_positive_features]\n",
    "ax1.barh(range(len(words_pos)), scores_pos, color='#2ecc71')\n",
    "ax1.set_yticks(range(len(words_pos)))\n",
    "ax1.set_yticklabels(words_pos, fontsize=10)\n",
    "ax1.set_xlabel('Coefficient', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Top 20 Kid-Friendly Features', fontsize=13, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Not Kid-Friendly features\n",
    "words_neg = [f[0] for f in top_negative_features]\n",
    "scores_neg = [f[1] for f in top_negative_features]\n",
    "ax2.barh(range(len(words_neg)), scores_neg, color='#e74c3c')\n",
    "ax2.set_yticks(range(len(words_neg)))\n",
    "ax2.set_yticklabels(words_neg, fontsize=10)\n",
    "ax2.set_xlabel('Coefficient', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Top 20 Not Kid-Friendly Features', fontsize=13, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature importance sauvegardée\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801654c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ANALYSE D'ERREURS\n",
    "# ============================================\n",
    "\n",
    "best_model = models['Logistic Regression']\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Erreurs\n",
    "errors = X_test[y_pred != y_test]\n",
    "errors_true = y_test[y_pred != y_test]\n",
    "errors_pred = y_pred[y_pred != y_test]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\" ANALYSE D'ERREURS - {len(errors)} erreurs sur {len(y_test)} ({len(errors)/len(y_test)*100:.2f}%)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# False Positives\n",
    "fp = errors[(errors_true == 0) & (errors_pred == 1)]\n",
    "print(f\"\\nFaux Positifs (prédit Kid-Friendly, réel Not): {len(fp)}\")\n",
    "if len(fp) > 0:\n",
    "    print(\"\\nExemples:\")\n",
    "    for i, text in enumerate(fp.head(3).values, 1):\n",
    "        print(f\"  {i}. {text[:100]}...\")\n",
    "\n",
    "# False Negatives\n",
    "fn = errors[(errors_true == 1) & (errors_pred == 0)]\n",
    "print(f\"\\nFaux Négatifs (prédit Not, réel Kid-Friendly): {len(fn)}\")\n",
    "if len(fn) > 0:\n",
    "    print(\"\\nExemples:\")\n",
    "    for i, text in enumerate(fn.head(3).values, 1):\n",
    "        print(f\"  {i}. {text[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Prochaine étape: 05_model_registry.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9342d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEEPCHECKS: MODEL PERFORMANCE VALIDATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" VALIDATION DEEPCHECKS - PERFORMANCE DES MODÈLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    from deepchecks.tabular import Dataset\n",
    "    from deepchecks.tabular.checks import (\n",
    "        ConfusionMatrixReport, RocReport, \n",
    "        SimpleModelComparison, PerformanceReport,\n",
    "        WeakSegmentsPerformance, ModelInferenceTime\n",
    "    )\n",
    "    \n",
    "    # Créer datasets\n",
    "    train_df_dc = pd.DataFrame({'keywords_text': X_train, 'is_kid_friendly': y_train})\n",
    "    test_df_dc = pd.DataFrame({'keywords_text': X_test, 'is_kid_friendly': y_test})\n",
    "    \n",
    "    train_ds = Dataset(train_df_dc, label='is_kid_friendly', cat_features=[])\n",
    "    test_ds = Dataset(test_df_dc, label='is_kid_friendly', cat_features=[])\n",
    "    \n",
    "    # Utiliser le meilleur modèle (Logistic Regression)\n",
    "    best_model = models['Logistic Regression']\n",
    "    \n",
    "    print(\"\\n1. Analyse de la matrice de confusion (Deepchecks)...\")\n",
    "    try:\n",
    "        result_cm = ConfusionMatrixReport().run(train_ds, test_ds, best_model)\n",
    "        print(\"   ✓ Matrice de confusion analysée\")\n",
    "        result_cm.show()\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Erreur: {str(e)[:60]}\")\n",
    "    \n",
    "    print(\"\\n2. Analyse ROC détaillée...\")\n",
    "    try:\n",
    "        result_roc = RocReport().run(train_ds, test_ds, best_model)\n",
    "        print(\"   ✓ Courbe ROC analysée\")\n",
    "        result_roc.show()\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Erreur: {str(e)[:60]}\")\n",
    "    \n",
    "    print(\"\\n3. Comparaison avec modèle simple...\")\n",
    "    try:\n",
    "        result_simple = SimpleModelComparison().run(train_ds, test_ds, best_model)\n",
    "        if result_simple.passed():\n",
    "            print(\"   ✓ Modèle surpasse le baseline simple\")\n",
    "        else:\n",
    "            print(\"   ⚠ Modèle ne surpasse pas significativement le baseline\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Erreur: {str(e)[:60]}\")\n",
    "    \n",
    "    print(\"\\n4. Rapport de performance global...\")\n",
    "    try:\n",
    "        result_perf = PerformanceReport().run(train_ds, test_ds, best_model)\n",
    "        print(\"   ✓ Performance globale analysée\")\n",
    "        result_perf.show()\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Erreur: {str(e)[:60]}\")\n",
    "    \n",
    "    print(\"\\n5. Détection des segments faibles...\")\n",
    "    try:\n",
    "        result_weak = WeakSegmentsPerformance().run(train_ds, test_ds, best_model)\n",
    "        if result_weak.passed():\n",
    "            print(\"   ✓ Performance uniforme sur tous les segments\")\n",
    "        else:\n",
    "            print(\"   ⚠ Segments faibles détectés\")\n",
    "            print(\"   → Certains types de podcasts ont des performances plus faibles\")\n",
    "        result_weak.show()\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Erreur: {str(e)[:60]}\")\n",
    "    \n",
    "    print(\"\\n6. Temps d'inférence du modèle...\")\n",
    "    try:\n",
    "        result_time = ModelInferenceTime().run(train_ds, test_ds, best_model)\n",
    "        if result_time.passed():\n",
    "            print(\"   ✓ Temps d'inférence acceptable\")\n",
    "        else:\n",
    "            print(\"   ⚠ Temps d'inférence élevé\")\n",
    "        print(f\"   → Temps moyen: {result_time.value:.4f}s par prédiction\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠ Erreur: {str(e)[:60]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" RÉSUMÉ VALIDATION MODÈLE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n ✓ Validation des performances terminée\")\n",
    "    print(\" → Les résultats détaillés sont affichés ci-dessus\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ℹ Modèle validé - Prêt pour le registry\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠ Deepchecks non installé\")\n",
    "    print(\"  → Installation: pip install deepchecks\")\n",
    "    print(\"  → Suite sans validation deepchecks\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Erreur deepchecks: {str(e)[:100]}\")\n",
    "    print(\"  → Suite sans validation deepchecks\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
