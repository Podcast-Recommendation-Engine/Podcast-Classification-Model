{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec539d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS ET CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ENVIRONNEMENT CONFIGURÉ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbb8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION MLFLOW\n",
    "# ============================================\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "MLFLOW_ENABLED = False\n",
    "\n",
    "def check_mlflow_connection(uri):\n",
    "    try:\n",
    "        response = requests.get(uri, timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" VÉRIFICATION MLFLOW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if check_mlflow_connection(MLFLOW_TRACKING_URI):\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"podcast-classification-kid-friendly\")\n",
    "    MLFLOW_ENABLED = True\n",
    "    print(f\" ✓ MLflow connecté: {MLFLOW_TRACKING_URI}\")\n",
    "else:\n",
    "    print(f\" ✗ MLflow non accessible\")\n",
    "    print(f\" → Pour activer: cd docker && docker-compose up -d\")\n",
    "    MLFLOW_ENABLED = False\n",
    "\n",
    "print(f\" MLflow: {'Activé' if MLFLOW_ENABLED else 'Désactivé'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHARGEMENT DES DONNÉES\n",
    "# ============================================\n",
    "\n",
    "df = pd.read_csv('../data/annotated/podcasts_annotated.csv')\n",
    "\n",
    "if 'keywords_text' not in df.columns:\n",
    "    df['keywords_text'] = df['keywords_clean'].apply(\n",
    "        lambda x: ' '.join(eval(x)) if isinstance(x, str) else ' '.join(x)\n",
    "    )\n",
    "\n",
    "print(f\"Dataset chargé: {len(df):,} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab910ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SPLIT TRAIN/TEST STRATIFIÉ\n",
    "# ============================================\n",
    "\n",
    "X = df['keywords_text']\n",
    "y = df['is_kid_friendly']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" SPLIT TRAIN/TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\" Train: {len(X_train):,} ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\" Test:  {len(X_test):,} ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25403911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DEEPCHECKS: TRAIN-TEST VALIDATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" VALIDATION DEEPCHECKS - TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    from deepchecks.tabular import Dataset\n",
    "    from deepchecks.tabular.checks import (\n",
    "        TrainTestLabelDrift, NewLabelTrainTest, \n",
    "        TrainTestSamplesMix, TrainTestFeatureDrift\n",
    "    )\n",
    "    \n",
    "    # Créer datasets Deepchecks\n",
    "    train_df_dc = pd.DataFrame({'keywords_text': X_train, 'is_kid_friendly': y_train})\n",
    "    test_df_dc = pd.DataFrame({'keywords_text': X_test, 'is_kid_friendly': y_test})\n",
    "    \n",
    "    train_ds = Dataset(train_df_dc, label='is_kid_friendly', cat_features=[])\n",
    "    test_ds = Dataset(test_df_dc, label='is_kid_friendly', cat_features=[])\n",
    "    \n",
    "    print(\"\\n1. Vérification du drift des labels...\")\n",
    "    result_label_drift = TrainTestLabelDrift().run(train_ds, test_ds)\n",
    "    if result_label_drift.passed():\n",
    "        print(\"   ✓ Distribution des labels similaire train/test\")\n",
    "    else:\n",
    "        print(\"   ⚠ Drift détecté dans la distribution des labels\")\n",
    "        print(f\"   → Peut affecter la généralisation du modèle\")\n",
    "    \n",
    "    print(\"\\n2. Vérification des nouveaux labels...\")\n",
    "    result_new_labels = NewLabelTrainTest().run(train_ds, test_ds)\n",
    "    if result_new_labels.passed():\n",
    "        print(\"   ✓ Pas de nouveaux labels dans test\")\n",
    "    else:\n",
    "        print(\"   ⚠ Nouveaux labels trouvés dans test set\")\n",
    "    \n",
    "    print(\"\\n3. Détection de fuite de données (data leakage)...\")\n",
    "    result_leakage = TrainTestSamplesMix().run(train_ds, test_ds)\n",
    "    if result_leakage.passed():\n",
    "        print(\"   ✓ Pas de fuite de données détectée\")\n",
    "    else:\n",
    "        print(\"   ⚠ Échantillons identiques train/test détectés (data leakage!)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" RÉSUMÉ VALIDATION TRAIN/TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    checks_results = [\n",
    "        (\"Label drift\", result_label_drift),\n",
    "        (\"Nouveaux labels\", result_new_labels),\n",
    "        (\"Data leakage\", result_leakage)\n",
    "    ]\n",
    "    \n",
    "    passed = sum(1 for _, r in checks_results if r.passed())\n",
    "    total = len(checks_results)\n",
    "    \n",
    "    print(f\"\\n Checks réussis: {passed}/{total}\")\n",
    "    \n",
    "    failed_checks = [name for name, r in checks_results if not r.passed()]\n",
    "    if failed_checks:\n",
    "        print(f\"\\n ⚠ Checks échoués:\")\n",
    "        for check in failed_checks:\n",
    "            print(f\"   • {check}\")\n",
    "        print(\"\\n → Recommandation: Vérifier le split train/test\")\n",
    "    else:\n",
    "        print(\"\\n ✓ Split train/test validé!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ℹ Train/Test validés - Prêt pour entraînement\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠ Deepchecks non installé\")\n",
    "    print(\"  → Installation: pip install deepchecks\")\n",
    "    print(\"  → Suite sans validation deepchecks\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Erreur deepchecks: {str(e)[:100]}\")\n",
    "    print(\"  → Suite sans validation deepchecks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DÉFINITION DES MODÈLES BASELINE\n",
    "# ============================================\n",
    "\n",
    "tfidf_params = {\n",
    "    'max_features': None,\n",
    "    'min_df': 2,\n",
    "    'max_df': 0.95,\n",
    "    'ngram_range': (1, 2),\n",
    "    'sublinear_tf': True\n",
    "}\n",
    "\n",
    "baseline_models = {\n",
    "    'Dummy (Baseline Naïve)': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
    "        ('clf', DummyClassifier(strategy='most_frequent', random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
    "        ('clf', LogisticRegression(\n",
    "            max_iter=1000, random_state=42, \n",
    "            class_weight='balanced', solver='liblinear'\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'Linear SVM': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
    "        ('clf', LinearSVC(\n",
    "            max_iter=2000, random_state=42,\n",
    "            class_weight='balanced', dual='auto'\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'Multinomial Naive Bayes': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
    "        ('clf', MultinomialNB(alpha=1.0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" MODÈLES BASELINE CONFIGURÉS\")\n",
    "print(\"=\"*80)\n",
    "for i, name in enumerate(baseline_models.keys(), 1):\n",
    "    print(f\"   {i}. {name}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b80227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FONCTION D'ENTRAÎNEMENT\n",
    "# ============================================\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, \n",
    "                             model_name, log_to_mlflow=None):\n",
    "    \"\"\"\n",
    "    Entraîne et évalue un modèle avec logging MLflow optionnel\n",
    "    \"\"\"\n",
    "    if log_to_mlflow is None:\n",
    "        log_to_mlflow = MLFLOW_ENABLED\n",
    "    \n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\" Entraînement: {model_name}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    # Démarrer MLflow run\n",
    "    if log_to_mlflow:\n",
    "        try:\n",
    "            mlflow.start_run(run_name=model_name)\n",
    "            mlflow.set_tag(\"model_type\", model_name)\n",
    "            mlflow.set_tag(\"framework\", \"scikit-learn\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠ MLflow: {str(e)[:50]}\")\n",
    "            log_to_mlflow = False\n",
    "    \n",
    "    try:\n",
    "        # Entraînement\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Prédictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Probabilités\n",
    "        y_pred_proba = None\n",
    "        try:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            elif hasattr(model, 'decision_function'):\n",
    "                y_pred_proba = model.decision_function(X_test)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Métriques\n",
    "        results = {\n",
    "            'model': model,\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1_score': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'train_time': train_time,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        if y_pred_proba is not None:\n",
    "            try:\n",
    "                results['roc_auc'] = roc_auc_score(y_test, y_pred_proba)\n",
    "            except:\n",
    "                results['roc_auc'] = np.nan\n",
    "        else:\n",
    "            results['roc_auc'] = np.nan\n",
    "        \n",
    "        # Log MLflow\n",
    "        if log_to_mlflow:\n",
    "            try:\n",
    "                mlflow.log_metric(\"accuracy\", results['accuracy'])\n",
    "                mlflow.log_metric(\"precision\", results['precision'])\n",
    "                mlflow.log_metric(\"recall\", results['recall'])\n",
    "                mlflow.log_metric(\"f1_score\", results['f1_score'])\n",
    "                mlflow.log_metric(\"train_time\", results['train_time'])\n",
    "                if not np.isnan(results['roc_auc']):\n",
    "                    mlflow.log_metric(\"roc_auc\", results['roc_auc'])\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠ Log error: {str(e)[:50]}\")\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\" Terminé en {train_time:.2f}s\")\n",
    "        print(f\"\\n Métriques:\")\n",
    "        print(f\"   • Accuracy:  {results['accuracy']:.4f}\")\n",
    "        print(f\"   • Precision: {results['precision']:.4f}\")\n",
    "        print(f\"   • Recall:    {results['recall']:.4f}\")\n",
    "        print(f\"   • F1-Score:  {results['f1_score']:.4f}\")\n",
    "        if not np.isnan(results['roc_auc']):\n",
    "            print(f\"   • ROC-AUC:   {results['roc_auc']:.4f}\")\n",
    "        \n",
    "        if log_to_mlflow and mlflow.active_run():\n",
    "            print(f\"\\n  ✓ MLflow Run: {mlflow.active_run().info.run_id[:8]}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    finally:\n",
    "        if log_to_mlflow and mlflow.active_run():\n",
    "            try:\n",
    "                mlflow.end_run()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENTRAÎNEMENT DES MODÈLES BASELINE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" PHASE 1: ENTRAÎNEMENT BASELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for model_name, model in baseline_models.items():\n",
    "    results = train_and_evaluate_model(\n",
    "        model, X_train, y_train, X_test, y_test, model_name\n",
    "    )\n",
    "    baseline_results[model_name] = results\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" PHASE 1 TERMINÉE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2228d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARAISON DES RÉSULTATS\n",
    "# ============================================\n",
    "\n",
    "df_results = pd.DataFrame([\n",
    "    {\n",
    "        'Modèle': name,\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1-Score': results['f1_score'],\n",
    "        'ROC-AUC': results['roc_auc'],\n",
    "        'Temps (s)': results['train_time']\n",
    "    }\n",
    "    for name, results in baseline_results.items()\n",
    "]).sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RÉSULTATS BASELINE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "best = df_results.iloc[0]\n",
    "print(f\"\\n MEILLEUR: {best['Modèle']}\")\n",
    "print(f\"   • F1-Score: {best['F1-Score']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CROSS-VALIDATION (optionnel)\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" CROSS-VALIDATION STRATIFIÉE (5-fold)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name, model_info in baseline_results.items():\n",
    "    if 'Dummy' in model_name:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for metric in ['f1', 'accuracy']:\n",
    "        scores = cross_val_score(\n",
    "            model_info['model'], X_train, y_train,\n",
    "            cv=skf, scoring=metric, n_jobs=-1\n",
    "        )\n",
    "        print(f\"   {metric.upper():10s}: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" Prochaine étape: 03_hyperparameter_tuning.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
