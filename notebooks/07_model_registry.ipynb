{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb7ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " MLflow Model Registry\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" MLflow Model Registry\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972f7d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking URI: http://localhost:5000\n",
      "Model Name: kid-friendly-classifier\n",
      "Experiment: podcast-classification-kid-friendly\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION MLFLOW\n",
    "# ============================================\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"\n",
    "MODEL_NAME = \"kid-friendly-classifier\"\n",
    "EXPERIMENT_NAME = \"podcast-classification-kid-friendly\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "client = MlflowClient()\n",
    "\n",
    "print(f\"Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88598ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " TOP 10 RUNS (par F1-Score)\n",
      "================================================================================\n",
      "\n",
      "        tags.model_type  metrics.f1_score  metrics.accuracy  metrics.precision  metrics.recall\n",
      "             Linear SVM          0.734694          0.845238           0.666667        0.818182\n",
      "    Logistic Regression          0.679245          0.797619           0.580645        0.818182\n",
      "Multinomial Naive Bayes          0.466667          0.809524           0.875000        0.318182\n",
      " Dummy (Baseline Naïve)          0.000000          0.738095           0.000000        0.000000\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# RECHERCHE DU MEILLEUR RUN\n",
    "# ============================================\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "\n",
    "if not experiment:\n",
    "    print(f\"⚠ Expérience '{EXPERIMENT_NAME}' introuvable\")\n",
    "    print(\" → Vérifiez que les notebooks précédents ont été exécutés\")\n",
    "else:\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"metrics.f1_score DESC\"],\n",
    "        max_results=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" TOP 10 RUNS (par F1-Score)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    display_cols = ['tags.model_type', 'metrics.f1_score', \n",
    "                   'metrics.accuracy', 'metrics.precision', 'metrics.recall']\n",
    "    available_cols = [col for col in display_cols if col in runs.columns]\n",
    "    \n",
    "    print(runs[available_cols].head(10).to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b64727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FONCTION D'ENREGISTREMENT\n",
    "# ============================================\n",
    "\n",
    "def select_and_register_best_model(\n",
    "    experiment_name,\n",
    "    model_name,\n",
    "    metric=\"f1_score\",\n",
    "    stage=\"Production\",\n",
    "    archive_existing=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Sélectionne le meilleur run et enregistre dans Model Registry\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" SÉLECTION ET ENREGISTREMENT DU MEILLEUR MODÈLE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Récupérer expérience\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        print(f\" ✗ Expérience '{experiment_name}' introuvable\")\n",
    "        return None\n",
    "    \n",
    "    # Rechercher le meilleur run\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[f\"metrics.{metric} DESC\"],\n",
    "        max_results=1\n",
    "    )\n",
    "    \n",
    "    if runs.empty:\n",
    "        print(\" ✗ Aucun run trouvé\")\n",
    "        return None\n",
    "    \n",
    "    best_run = runs.iloc[0]\n",
    "    run_id = best_run['run_id']\n",
    "    \n",
    "    print(f\"\\n Meilleur Run ID: {run_id[:8]}...\")\n",
    "    print(f\" Modèle: {best_run.get('tags.model_type', 'Unknown')}\")\n",
    "    print(f\" {metric.upper()}: {best_run[f'metrics.{metric}']:.4f}\")\n",
    "    \n",
    "    # Métriques pour la description\n",
    "    metrics = {}\n",
    "    for m in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n",
    "        col = f'metrics.{m}'\n",
    "        if col in best_run:\n",
    "            metrics[m] = best_run[col]\n",
    "    \n",
    "    # Description de version\n",
    "    description = f\"\"\"Best performing model selected on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "Performance Metrics:\n",
    "{chr(10).join([f'  • {k.replace('_', ' ').title()}: {v:.4f}' for k, v in metrics.items()])}\n",
    "\n",
    "Model Type: {best_run.get('tags.model_type', 'Unknown')}\n",
    "Training Date: {best_run.get('start_time', 'Unknown')}\n",
    "\"\"\"\n",
    "    \n",
    "    # Enregistrer le modèle\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    \n",
    "    try:\n",
    "        model_details = mlflow.register_model(\n",
    "            model_uri=model_uri,\n",
    "            name=model_name\n",
    "        )\n",
    "        \n",
    "        version = model_details.version\n",
    "        print(f\"\\n ✓ Modèle enregistré: {model_name} v{version}\")\n",
    "        \n",
    "        # Mise à jour de la description\n",
    "        client = MlflowClient()\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            description=description\n",
    "        )\n",
    "        \n",
    "        # Tags\n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"model_type\",\n",
    "            value=str(best_run.get('tags.model_type', 'Unknown'))\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"validation_f1_score\",\n",
    "            value=str(best_run[f'metrics.{metric}'])\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"training_date\",\n",
    "            value=datetime.now().strftime('%Y-%m-%d')\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            key=\"framework\",\n",
    "            value=\"scikit-learn\"\n",
    "        )\n",
    "        \n",
    "        print(\" ✓ Métadonnées mises à jour\")\n",
    "        \n",
    "        # Transition vers Production\n",
    "        if archive_existing:\n",
    "            # Archiver les versions en Production\n",
    "            existing_versions = client.get_latest_versions(\n",
    "                name=model_name,\n",
    "                stages=[\"Production\"]\n",
    "            )\n",
    "            for v in existing_versions:\n",
    "                client.transition_model_version_stage(\n",
    "                    name=model_name,\n",
    "                    version=v.version,\n",
    "                    stage=\"Archived\"\n",
    "                )\n",
    "                print(f\" → Version {v.version} archivée\")\n",
    "        \n",
    "        # Nouvelle version en Production\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=version,\n",
    "            stage=stage\n",
    "        )\n",
    "        print(f\" ✓ Version {version} → {stage}\")\n",
    "        \n",
    "        # Alias\n",
    "        try:\n",
    "            client.set_registered_model_alias(\n",
    "                name=model_name,\n",
    "                alias=\"champion\",\n",
    "                version=version\n",
    "            )\n",
    "            print(f\" ✓ Alias 'champion' assigné à v{version}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" ENREGISTREMENT RÉUSSI\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'version': version,\n",
    "            'run_id': run_id,\n",
    "            'stage': stage,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" ✗ Erreur: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d60e1a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " SÉLECTION ET ENREGISTREMENT DU MEILLEUR MODÈLE\n",
      "================================================================================\n",
      "\n",
      " Meilleur Run ID: d6f8d9c6...\n",
      " Modèle: Linear SVM\n",
      " F1_SCORE: 0.7347\n",
      " ✗ Erreur: API request to endpoint /api/2.0/mlflow/logged-models/search failed with error code 404 != 200. Response body: '<!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'kid-friendly-classifier'.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENREGISTREMENT\n",
    "# ============================================\n",
    "\n",
    "result = select_and_register_best_model(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    model_name=MODEL_NAME,\n",
    "    metric=\"f1_score\",\n",
    "    stage=\"Production\",\n",
    "    archive_existing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b3984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSTRUCTIONS DE CHARGEMENT\n",
    "# ============================================\n",
    "\n",
    "if result:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" INSTRUCTIONS DE CHARGEMENT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n1. Charger par STAGE (Production):\")\n",
    "    print(f\"\"\"```python\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"{MLFLOW_TRACKING_URI}\")\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/{MODEL_NAME}/Production\")\n",
    "predictions = model.predict(new_data)\n",
    "```\"\"\")\n",
    "    \n",
    "    print(f\"\\n2. Charger par VERSION (v{result['version']}):\")\n",
    "    print(f\"\"\"```python\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"{MLFLOW_TRACKING_URI}\")\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/{MODEL_NAME}/{result['version']}\")\n",
    "predictions = model.predict(new_data)\n",
    "```\"\"\")\n",
    "    \n",
    "    print(f\"\\n3. Charger par ALIAS (champion):\")\n",
    "    print(f\"\"\"```python\n",
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"{MLFLOW_TRACKING_URI}\")\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/{MODEL_NAME}@champion\")\n",
    "predictions = model.predict(new_data)\n",
    "```\"\"\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ACCÈS UI MLFLOW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n → Interface: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\" → Expérience: {EXPERIMENT_NAME}\")\n",
    "    print(f\" → Modèle: {MODEL_NAME} (v{result['version']})\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ✓ PROJET TERMINÉ\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8922526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " TEST DE CHARGEMENT DU MODÈLE\n",
      "================================================================================\n",
      " ✗ Erreur: No versions of model with name 'kid-friendly-classifier' and stage 'Production' found\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TEST DE CHARGEMENT\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" TEST DE CHARGEMENT DU MODÈLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    model = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}/Production\")\n",
    "    print(f\" ✓ Modèle chargé: {MODEL_NAME}/Production\")\n",
    "    \n",
    "    # Test avec exemples\n",
    "    test_data = [\n",
    "        \"kids children education fun learning\",\n",
    "        \"crime murder mystery dark violence\"\n",
    "    ]\n",
    "    \n",
    "    predictions = model.predict(test_data)\n",
    "    \n",
    "    print(\"\\n Prédictions de test:\")\n",
    "    for i, (text, pred) in enumerate(zip(test_data, predictions), 1):\n",
    "        label = \"Kid-Friendly\" if pred == 1 else \"Not Kid-Friendly\"\n",
    "        print(f\"   {i}. '{text[:40]}...' → {label}\")\n",
    "    \n",
    "    print(\"\\n ✓ Modèle fonctionnel\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\" ✗ Erreur: {str(e)}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Podcast-Classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
