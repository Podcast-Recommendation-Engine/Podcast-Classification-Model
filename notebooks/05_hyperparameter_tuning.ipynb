{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, ParameterGrid\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" ENVIRONNEMENT CONFIGURÉ POUR TUNING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MLFLOW SETUP\n",
    "# ============================================\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"podcast-classification-tuning\")\n",
    "\n",
    "# Set tracking URI (optional - defaults to ./mlruns)\n",
    "# mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" MLFLOW CONFIGURED\")\n",
    "print(f\" Experiment: podcast-classification-tuning\")\n",
    "print(f\" Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHARGEMENT DES DONNÉES\n",
    "# ============================================\n",
    "\n",
    "df = pd.read_csv('../data/annotated/podcasts_annotated.csv')\n",
    "\n",
    "if 'keywords_text' not in df.columns:\n",
    "    df['keywords_text'] = df['keywords_clean'].apply(\n",
    "        lambda x: ' '.join(eval(x)) if isinstance(x, str) else ' '.join(x)\n",
    "    )\n",
    "\n",
    "X = df['keywords_text']\n",
    "y = df['is_kid_friendly']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train):,} | Test: {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bc4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODÈLES BASELINE (pour référence)\n",
    "# ============================================\n",
    "\n",
    "tfidf_params = {\n",
    "    'max_features': None,\n",
    "    'min_df': 2,\n",
    "    'max_df': 0.95,\n",
    "    'ngram_range': (1, 2),\n",
    "    'sublinear_tf': True\n",
    "}\n",
    "\n",
    "baseline_models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
    "        ('clf', LogisticRegression(\n",
    "            max_iter=1000, random_state=42,\n",
    "            class_weight='balanced', solver='liblinear'\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'Linear SVM': Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(**tfidf_params)),\n",
    "        ('clf', LinearSVC(\n",
    "            max_iter=2000, random_state=42,\n",
    "            class_weight='balanced', dual='auto'\n",
    "        ))\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ae8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GRILLES DE PARAMÈTRES\n",
    "# ============================================\n",
    "\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'tfidf__min_df': [1, 2, 3],\n",
    "        'tfidf__max_df': [0.9, 0.95, 1.0],\n",
    "        'clf__C': [0.1, 1.0, 10.0],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__class_weight': ['balanced']\n",
    "    },\n",
    "    \n",
    "    'Linear SVM': {\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'tfidf__min_df': [1, 2, 3],\n",
    "        'tfidf__max_df': [0.9, 0.95, 1.0],\n",
    "        'clf__C': [0.1, 1.0, 10.0],\n",
    "        'clf__class_weight': ['balanced']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" GRILLES DE PARAMÈTRES DÉFINIES\")\n",
    "print(\"=\"*80)\n",
    "for model_name, grid in param_grids.items():\n",
    "    n_comb = len(list(ParameterGrid(grid)))\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"   • Combinaisons: {n_comb}\")\n",
    "    print(f\"   • Total fits (5-fold): {n_comb * 5}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FONCTION DE TUNING\n",
    "# ============================================\n",
    "\n",
    "def tune_model(model, param_grid, X_train, y_train, model_name, cv=5):\n",
    "    \"\"\"\n",
    "    Fine-tuning avec GridSearchCV + MLflow tracking\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'─'*80}\")\n",
    "    print(f\" Fine-Tuning: {model_name}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    n_combinations = len(list(ParameterGrid(param_grid)))\n",
    "    print(f\"Configuration: {n_combinations} combinaisons × {cv}-fold = {n_combinations * cv} fits\")\n",
    "    print(f\"Métrique: F1-Score\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grid,\n",
    "        cv=StratifiedKFold(cv, shuffle=True, random_state=42),\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    tuning_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n Terminé en {tuning_time:.2f}s ({tuning_time/60:.1f} min)\")\n",
    "    print(f\" Meilleur F1 CV: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"\\n Meilleurs paramètres:\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"   • {param:30s}: {value}\")\n",
    "    \n",
    "    # MLflow tracking\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_tuning\"):\n",
    "        # Log tags\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.set_tag(\"stage\", \"hyperparameter_tuning\")\n",
    "        \n",
    "        # Log hyperparameters\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            mlflow.log_param(param, value)\n",
    "        \n",
    "        # Log tuning configuration\n",
    "        mlflow.log_param(\"cv_folds\", cv)\n",
    "        mlflow.log_param(\"n_combinations\", n_combinations)\n",
    "        mlflow.log_param(\"scoring\", \"f1\")\n",
    "        \n",
    "        # Log CV metrics\n",
    "        mlflow.log_metric(\"best_cv_f1_score\", grid_search.best_score_)\n",
    "        mlflow.log_metric(\"tuning_time_seconds\", tuning_time)\n",
    "        \n",
    "        # Log the best model\n",
    "        mlflow.sklearn.log_model(grid_search.best_estimator_, \"model\")\n",
    "        \n",
    "        print(f\" ✓ Logged to MLflow (Run ID: {mlflow.active_run().info.run_id})\")\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_, tuning_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e58a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXÉCUTION DU TUNING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" PHASE: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n⏱ Cette phase peut prendre plusieurs minutes...\\n\")\n",
    "\n",
    "tuned_models = {}\n",
    "tuning_results = {}\n",
    "\n",
    "for model_name in ['Logistic Regression', 'Linear SVM']:\n",
    "    best_model, best_params, best_score, tune_time = tune_model(\n",
    "        baseline_models[model_name],\n",
    "        param_grids[model_name],\n",
    "        X_train, y_train,\n",
    "        model_name\n",
    "    )\n",
    "    \n",
    "    tuned_models[f\"{model_name} (Tuned)\"] = best_model\n",
    "    tuning_results[f\"{model_name} (Tuned)\"] = {\n",
    "        'best_params': best_params,\n",
    "        'best_cv_score': best_score,\n",
    "        'tuning_time': tune_time\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" TUNING TERMINÉ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70832a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ÉVALUATION SUR TEST SET\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" ÉVALUATION DES MODÈLES OPTIMISÉS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "tuned_test_results = {}\n",
    "\n",
    "for model_name, model in tuned_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    tuned_test_results[model_name] = results\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"   • Accuracy:  {results['accuracy']:.4f}\")\n",
    "    print(f\"   • Precision: {results['precision']:.4f}\")\n",
    "    print(f\"   • Recall:    {results['recall']:.4f}\")\n",
    "    print(f\"   • F1-Score:  {results['f1_score']:.4f}\")\n",
    "    \n",
    "    # Log test metrics to MLflow\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_test_evaluation\"):\n",
    "        mlflow.set_tag(\"model_type\", model_name)\n",
    "        mlflow.set_tag(\"stage\", \"test_evaluation\")\n",
    "        \n",
    "        # Log test metrics\n",
    "        mlflow.log_metric(\"test_accuracy\", results['accuracy'])\n",
    "        mlflow.log_metric(\"test_precision\", results['precision'])\n",
    "        mlflow.log_metric(\"test_recall\", results['recall'])\n",
    "        mlflow.log_metric(\"test_f1_score\", results['f1_score'])\n",
    "        \n",
    "        # Log dataset info\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eeae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RÉSUMÉ COMPARATIF\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" RÉSUMÉ DU TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for base_name in ['Logistic Regression', 'Linear SVM']:\n",
    "    tuned_name = f\"{base_name} (Tuned)\"\n",
    "    \n",
    "    cv_score = tuning_results[tuned_name]['best_cv_score']\n",
    "    test_score = tuned_test_results[tuned_name]['f1_score']\n",
    "    \n",
    "    print(f\"\\n{base_name}:\")\n",
    "    print(f\"   • Meilleur F1 CV:  {cv_score:.4f}\")\n",
    "    print(f\"   • F1 Test Set:     {test_score:.4f}\")\n",
    "    print(f\"   • Temps tuning:    {tuning_results[tuned_name]['tuning_time']:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" MLFLOW TRACKING\")\n",
    "print(\"=\"*80)\n",
    "print(f\" Tous les runs sont enregistrés dans: {mlflow.get_tracking_uri()}\")\n",
    "print(f\" Pour visualiser: mlflow ui\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n Prochaine étape: 06_model_evaluation.ipynb\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
